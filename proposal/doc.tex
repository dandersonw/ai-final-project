\documentclass{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{listings}
\usepackage{multicol}
\usepackage{url}

\setlength{\parindent}{4em}
\setlength{\parskip}{1em}

\title{Project Proposal}
\date{2018-10-31}
\author{Derick Anderson and Zack Reiter}

\begin{document}

\pagenumbering{gobble}
%% \maketitle
%% \newpage
%% \pagenumbering{arabic}

\begin{center}
  \textbf{Project Proposal}

  \textit{Derick Anderson and Zach Reiter}
\end{center}

\section*{Problem Description}

We will be classifying playing cards from the game Magic the Gathering (MtG).
One approach will use a card's art as input,
treating the problem
as essentially an image classification task.
The other approach will use the name of the card,
treating the problem
as essentially a text classification task.
The output will be one of four classes:
creature, instant, artifact, or enchantment.
These classes correspond to
finer designations the cards possess
rooted in game mechanics. These designations are
tied to certain ideas that are distinct from each other
and we want to learn whether these distinctions are
more consistently communicated through words or images.

We plan to also explore
how the model generalizes across different splits
of the whole corpus of MtG cards.
Particularly,
there exists a concept of ``sets'':
groups of cards released together that share thematic content and artists.

The problem is interesting because we will get the chance to
explore if the algorithms can learn generalizable features
of cards across different sets,
and to compare the performance based on the text and artwork.

\section*{Algorithms}

For both the image and text classification tasks
we plan to create deep learning models
composed of a pretrained portion
(from some publicly available source)
and a standard layer on top that will be trained on our data.

Zack will handle the image classification approach.
He will use a Inception V3
\cite{rethinking-the-inception-architecture}
model trained on ImageNet as a base.
Such a pretrained model
will have already learned the basic features of many images,
including curves, lines, and so forth.
We will replace the last layer of the Inception model
with one appropriate for the number of classes we have
and fine-tune the model.
The basic idea of taking a pretrained ImageNet model
and fine tuning it for image classification is well understood
in recent years.

Derick will handle the text classification approach.
He will use the pretrained character embeddings
released with Google's One Billion Word Benchmark
\cite{one-billion-words}.
It might be better if we could leverage a more comprehensive pretrained model,
but the text on MtG cards is only English-like -- not English.
On top of the character embeddings
we will use a standard RNN architecture.
Embedding plus RNN is a well understood architecture for text classification.

We can find no case of someone else using these algorithms for our problem.
There are a few cases of people using character based models
to generate MtG cards.
There is one notable case
\footnote{\url{https://github.com/hollisn/MagicTCG_Classification}}
of using the full text of a card to predict the card's ``color''.
We can find no case of anyone using the art on the cards.

\section*{Methodology}

From the set of all MtG cards we will define three collections.
MtG ``sets'' are released every
3 to 4 months and each have an individual style.
However,
the art direction shifts drastically at the set Shards of Alara,
going from more cartoonish and animated to more realistic.
Collection 1 will consist of cards from the most recent set,
``Guilds of Ravnica'' to ``Shards of Alara''.
That represents 54 sets at about 200 cards each,
for about 10,800 cards total.
Collection 2 will consist of sets earlier than ``Shards of Alara''
going back to one of the earliest sets, ``Arabian Nights''.
That is 52 sets having about 10,400 cards.
Collection 3 will be the union of the other two collections.
This will feature 106 sets or about 21,200 cards total.

For evaluation purposes
we will hold out ten percent of cards uniformly at random
from all the sets considered
before dividing into collections.
For a model trained on the earlier cards,
evaluating on earlier cards will be ``in-genre''
and on later cards will be ``out-of-genre''.
For the model trained on Collection 3
all cards will be in-genre.
We will train our model on each collection,
and report scores on the evaluation data
broken down by in-genre or out-of-genre.

\section*{Results}

We anticipate that the model will
consistently perform better on in-genre data.
Text classification
will likely perform better than image classification
on out-of-genre data.
Although MtG card names have many sparsely present proper nouns
(viz. fantasy nations or races),
they also have some consistent patterns based on common nouns
(soldier, scout, etc) and these patterns will form a more
secure basis for classification than images.
We hypothesize that text classification will perform
better than image classification on in-genre data as
well for similar reasons.

%% \begin{multicols}{2}

%% \end{multicols}
\bibliography{doc}{}
\bibliographystyle{plain}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
